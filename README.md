# HAI_Project: Medical QA with EXAONE 3.5 7.8B-Instruct

λ³Έ ν”„λ΅μ νΈλ” EXAONE 3.5 7.8B-Instruct λ¨λΈμ„ κΈ°λ°μΌλ΅ ν• ν•κµ­μ–΄ μλ£ QA νμΈνλ‹ ν”„λ΅μ νΈμ…λ‹λ‹¤.  
μ§λ¬Έ-μ§€μ‹-μ‘λ‹µ ν¬λ§·μ λ°μ΄ν„°μ…‹μ„ ν™μ©ν•΄ μλ£ λ¶„μ•Όμ μ •λ‹µλ¥ κ³Ό μ„¤λ… λ¥λ ¥μ„ ν–¥μƒμ‹ν‚¤λ” κ²ƒμ„ λ©ν‘λ΅ ν•©λ‹λ‹¤.

---
β… λ¨λΈ λ³€κ²½ μ‚¬μ 

λ³Έ ν”„λ΅μ νΈλ” μ΄κΈ°μ—λ” Upstage/SOLAR-10.7B-Instruct-v1.0 λ¨λΈμ„ κΈ°λ°μΌλ΅ μλ£ μ§μμ‘λ‹µ νμΈνλ‹μ„ μ§„ν–‰ν•μ€μµλ‹λ‹¤.

ν•μ§€λ§ λ‹¤μκ³Ό κ°™μ€ μ΄μ λ΅ ν„μ¬ μ‚¬μ© μ¤‘μΈ λ¨λΈλ΅ μ „ν™ν•κ² λμ—μµλ‹λ‹¤:

μ¶”λ΅  μ†λ„ κ°μ„ SOLAR-10.7B λ¨λΈμ€ νλΌλ―Έν„° μκ°€ λ§μ•„ μ¶”λ΅  μ‹κ°„μ΄ λ‹¤μ† κΈΈμ—μµλ‹λ‹¤.μ΄μ— λ”°λΌ λ³΄λ‹¤ κ²½λ‰ν™”λ κµ¬μ΅°λ¥Ό κ°–μ¶ λ¨λΈλ΅ κµμ²΄ν•μ—¬ μ‘λ‹µ μ†λ„λ¥Ό λ€ν­ ν–¥μƒμ‹μΌ°μµλ‹λ‹¤.

ν•κµ­μ–΄ νΉν™” μ„±λ¥ ν™•λ³΄SOLAR λ¨λΈμ€ λ‹¤κµ­μ–΄ μ²λ¦¬μ— κ°•μ μ΄ μμ—μ§€λ§,ν•κµ­μ–΄μ— νΉν™”λ μ‘λ‹µ ν’μ§μ„ μ¶©λ¶„ν λ³΄μ¥ν•μ§€λ” λ»ν–μµλ‹λ‹¤.ν„μ¬ μ‚¬μ©ν•λ” λ¨λΈμ€ ν•κµ­μ–΄μ— μµμ ν™”λ ν•™μµμ΄ μ„ ν–‰λμ–΄,λ³„λ„μ ν•κµ­μ–΄ SFT μ—†μ΄λ„ λ†’μ€ μ •λ‹µλ¥ μ„ λ³΄μ¥ν•©λ‹λ‹¤.

λν• κΈ°μ΅΄μ 1μ°¨ ν•™μµ λ°μ΄ν„°(S_F1)λ” μ μ™Έν•κ³ ,μλ£ QAμ— νΉν™”λ 2μ°¨ μ •μ  λ°μ΄ν„°μ…‹λ§μ„ μ‚¬μ©ν•μ—¬ νμΈνλ‹μ„ μ§„ν–‰ν•μ€μµλ‹λ‹¤.μ΄λ¥Ό ν†µν•΄ λ¨λΈμ μΌκ΄€μ„±κ³Ό μ‘λ‹µ μ‹ λΆ°λ„λ¥Ό λ”μ± λ†’μΌ μ μμ—μµλ‹λ‹¤.

π§Ύ ν•™μµ λ°μ΄ν„° κµ¬μ„±

μ‚¬μ©λ λ°μ΄ν„°μ…‹μ€ μ΄ 42λ§ μμ μ§λ¬Έ-μ‘λ‹µ μƒν”λ΅ κµ¬μ„±λμ–΄ μμΌλ©°,AI Hubμ—μ„ μ κ³µν• μλ£ μ§μμ‘λ‹µ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ μ•„λμ™€ κ°™μ€ νΈλ¦¬ κµ¬μ΅°λ¥Ό λ”°λ¦…λ‹λ‹¤:

νμΌ κµ¬μ΅°: 02.λΌλ²¨λ§λ°μ΄ν„°/TL/

π“ 02.λΌλ²¨λ§λ°μ΄ν„°/TL/
β”β”€β”€ π“ 1.μ§λ¬Έ/
β”‚   β”β”€β”€ π“ κ°κΈ°/
β”‚   β”‚   β”β”€β”€ π“ μλ°©/
β”‚   β”‚   β”‚   β”β”€β”€ q1.json
β”‚   β”‚   β”‚   β””β”€β”€ q2.json
β”‚   β”‚   β””β”€β”€ π“ μ¦μƒ/
β”‚   β”‚       β””β”€β”€ q1.json
β”‚   β”β”€β”€ π“ κ³ νμ••/
β”‚   β”‚   β”β”€β”€ π“ μ›μΈ/
β”‚   β”‚   β”β”€β”€ π“ μ§„λ‹¨/
β”‚   β”‚   β””β”€β”€ π“ μΉλ£/
β”‚   β””β”€β”€ ...
β”β”€β”€ π“ 2.μ‘λ‹µ/
β”‚   β”β”€β”€ π“ κ°κΈ°/
β”‚   β”‚   β”β”€β”€ π“ μλ°©/
β”‚   β”‚   β”‚   β”β”€β”€ a1.json
β”‚   β”‚   β”‚   β””β”€β”€ a2.json
β”‚   β”‚   β””β”€β”€ π“ μ¦μƒ/
β”‚   β”‚       β””β”€β”€ a1.json
β”‚   β”β”€β”€ π“ κ³ νμ••/
β”‚   β”‚   β”β”€β”€ π“ μ›μΈ/
β”‚   β”‚   β”β”€β”€ π“ μ§„λ‹¨/
β”‚   β”‚   β””β”€β”€ π“ μΉλ£/
β”‚   β””β”€β”€ ...
...
(μ΄ μ•½ 9,538κ°μ ν΄λ”)

μƒμ„ ν΄λ”λ” μ§λ³‘λ… (μ: κ³ νμ••, λ‹Ήλ‡¨λ³‘, κ°κΈ° λ“±)

ν•μ„ ν΄λ”λ” μ„Έλ¶€ μΉ΄ν…κ³ λ¦¬ (μ: μλ°©, μ›μΈ, μ¦μƒ, μ§„λ‹¨, μΉλ£ λ“±)

1.μ§λ¬Έ/μ—λ” κ° ν•­λ©μ— ν•΄λ‹Ήν•λ” μ§λ¬Έ JSON νμΌ, 2.μ‘λ‹µ/μ—λ” ν•΄λ‹Ή μ§λ¬Έμ— λ€ν• μ‘λ‹µ JSON νμΌμ΄ ν¬ν•¨λ©λ‹λ‹¤.

μ΄ κµ¬μ΅°λ¥Ό flattenν•μ—¬ SFTμ— μ ν•©ν• ν•μ‹μΌλ΅ μ „μ²λ¦¬ν•κ³ ,{"instruction": "...", "output": "..."} ν•νƒλ΅ ν•™μµμ— ν™μ©ν•μ€μµλ‹λ‹¤.

π§ λ°μ΄ν„° μ „μ²λ¦¬ μ¤ν¬λ¦½νΈ

preprocess_medical.pyλ” AI Hub TL κµ¬μ΅°μ μ§λ¬Έ/μ‘λ‹µ ν΄λ”λ¥Ό μνν•λ©°,
κ° μ§λ¬Έ(JSON)κ³Ό μ‘λ‹µ(JSON)μ„ λ§¤μΉ­ν• ν›„ instruction / output ν•μ‹μΌλ΅ κµ¬μ„±ν•μ—¬ .jsonl νμΌλ΅ μ €μ¥ν•λ” μ „μ²λ¦¬ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

μ: formatted_medical_dataset.jsonl
{"instruction": "κ³ νμ•• ν™μκ°€ ν”Όν•΄μ•Ό ν•  μμ‹μ€ λ¬΄μ—‡μΈκ°€μ”?", "output": "κΉ€μΉ, μ§  μμ‹ λ“± λ‚νΈλ¥¨μ΄ λ§μ€ μμ‹."}

μ „μ²λ¦¬λ μ΄ νμΌμ€ S_F2.pyμ ν•™μµ μ…λ ¥μΌλ΅ μ‚¬μ©λ©λ‹λ‹¤.

β™οΈ νμΈνλ‹ κµ¬μ„± μ¤ν¬λ¦½νΈ

S_F2.pyλ” λ³€κ²½λ EXAONE κΈ°λ° λ¨λΈμ— λ€ν•΄ 2μ°¨ μ •μ  λ°μ΄ν„°μ…‹μ„ ν™μ©ν• LoRA κΈ°λ° νμΈνλ‹μ„ μν–‰ν•λ” λ©”μΈ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.ν•΄λ‹Ή μ¤ν¬λ¦½νΈλ” λ‹¤μμ„ ν¬ν•¨ν•©λ‹λ‹¤:

4bit QLoRA μ„Έν…μ„ ν¬ν•¨ν• λ¨λΈ λ΅λ“

ν•™μµ λ°μ΄ν„° μ „μ²λ¦¬

Trainer κΈ°λ° ν•™μµ λ£¨ν”„

safetensors ν•μ‹ μ €μ¥ λ° Hugging Face Hub push

preprocess_medical.pyλ” AI Hub TL κµ¬μ΅°μ μ§λ¬Έ/μ‘λ‹µ ν΄λ”λ¥Ό μνν•λ©°,
κ° μ§λ¬Έ(JSON)κ³Ό μ‘λ‹µ(JSON)μ„ λ§¤μΉ­ν• ν›„ instruction / output ν•μ‹μΌλ΅ κµ¬μ„±ν•μ—¬ .jsonl νμΌλ΅ μ €μ¥ν•λ” μ „μ²λ¦¬ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

μ: formatted_medical_dataset.jsonl
{"instruction": "κ³ νμ•• ν™μκ°€ ν”Όν•΄μ•Ό ν•  μμ‹μ€ λ¬΄μ—‡μΈκ°€μ”?", "output": "κΉ€μΉ, μ§  μμ‹ λ“± λ‚νΈλ¥¨μ΄ λ§μ€ μμ‹."}

μ „μ²λ¦¬λ μ΄ νμΌμ€ S_F2.pyμ ν•™μµ μ…λ ¥μΌλ΅ μ‚¬μ©λ©λ‹λ‹¤.




## π“ λ¨λΈ μ •λ³΄

- **Base Model**: [`LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct`](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct)
- **νμΈνλ‹ λ°©μ‹**: QLoRA (4bit μ–‘μν™” + LoRA)
- **λ°μ΄ν„° ν•μ‹**: `{"instruction": "...", "output": "..."}` (JSONL)
- **μ‚¬μ© λ¶„μ•Ό**: ν•κµ­μ–΄ μλ£ μ§μμ‘λ‹µ, μ„¤λ…ν• QA, λ‹¨λ‹µν• μ„ νƒ λ¬Έμ 

---

## π“‚ ν•™μµ λ°μ΄ν„°

- μ‚¬μ© λ°μ΄ν„°μ…‹: `formatted_medical_dataset.jsonl`
- μ£Όμ” κµ¬μ„±:  
  - `instruction`: μ§λ¬Έ λ° μ§€μ‹ λ¬Έμ¥  
  - `output`: μ •λ‹µ λλ” μλ£μ  μ„¤λ… μ‘λ‹µ

---

## β™οΈ ν•™μµ μ„Έλ¶€ μ„¤μ •

- `per_device_train_batch_size`: 2  
- `gradient_accumulation_steps`: 4  
- `num_train_epochs`: 3  
- `bnb_4bit`: NF4, double quant  
- `LoRA target modules`: `["q_proj", "k_proj", "v_proj", "o_proj"]`

---

## π§  μ‚¬μ© μμ‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("HongKi08/HAI_Project")
tokenizer = AutoTokenizer.from_pretrained("HongKi08/HAI_Project")

prompt = "κ³ νμ•• ν™μκ°€ ν”Όν•΄μ•Ό ν•  μμ‹μ€ λ¬΄μ—‡μΈκ°€μ”?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
