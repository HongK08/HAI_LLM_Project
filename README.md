# HAI_Project: Medical QA with EXAONE 3.5 7.8B-Instruct

ë³¸ í”„ë¡œì íŠ¸ëŠ” EXAONE 3.5 7.8B-Instruct ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í•œêµ­ì–´ ì˜ë£Œ QA íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
ì§ˆë¬¸-ì§€ì‹œ-ì‘ë‹µ í¬ë§·ì˜ ë°ì´í„°ì…‹ì„ í™œìš©í•´ ì˜ë£Œ ë¶„ì•¼ì˜ ì •ë‹µë¥ ê³¼ ì„¤ëª… ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## ğŸ“Œ ëª¨ë¸ ì •ë³´

- **Base Model**: [`LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct`](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct)
- **íŒŒì¸íŠœë‹ ë°©ì‹**: QLoRA (4bit ì–‘ìí™” + LoRA)
- **ë°ì´í„° í˜•ì‹**: `{"instruction": "...", "output": "..."}` (JSONL)
- **ì‚¬ìš© ë¶„ì•¼**: í•œêµ­ì–´ ì˜ë£Œ ì§ˆì˜ì‘ë‹µ, ì„¤ëª…í˜• QA, ë‹¨ë‹µí˜• ì„ íƒ ë¬¸ì œ

---

## ğŸ“‚ í•™ìŠµ ë°ì´í„°

- ì‚¬ìš© ë°ì´í„°ì…‹: `formatted_medical_dataset.jsonl`
- ì£¼ìš” êµ¬ì„±:  
  - `instruction`: ì§ˆë¬¸ ë° ì§€ì‹œ ë¬¸ì¥  
  - `output`: ì •ë‹µ ë˜ëŠ” ì˜ë£Œì  ì„¤ëª… ì‘ë‹µ

---

## âš™ï¸ í•™ìŠµ ì„¸ë¶€ ì„¤ì •

- `per_device_train_batch_size`: 2  
- `gradient_accumulation_steps`: 4  
- `num_train_epochs`: 3  
- `bnb_4bit`: NF4, double quant  
- `LoRA target modules`: `["q_proj", "k_proj", "v_proj", "o_proj"]`

---

## ğŸ§  ì‚¬ìš© ì˜ˆì‹œ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("HongKi08/HAI_Project")
tokenizer = AutoTokenizer.from_pretrained("HongKi08/HAI_Project")

prompt = "ê³ í˜ˆì•• í™˜ìê°€ í”¼í•´ì•¼ í•  ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
